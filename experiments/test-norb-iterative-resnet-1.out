Files already downloaded and verified
Files already downloaded and verified
Lottery Ticket Experiment for <function ResNet18 at 0x7f00f7354840> with pruning rate 10.737418240000006 on device cuda:3
Training base network...
Starting epoch 1
[epoch:1,batch:100]: loss(train): 1.842, acc(train): 0.453, loss(val): 1.652, acc(val): 0.388
[epoch:1,batch:200]: loss(train): 2.717, acc(train): 0.070, loss(val): 2.301, acc(val): 0.100
[epoch:1,batch:300]: loss(train): 2.290, acc(train): 0.203, loss(val): 2.284, acc(val): 0.138
Starting epoch 2
[epoch:2,batch:100]: loss(train): 3.083, acc(train): 0.211, loss(val): 1.994, acc(val): 0.297
[epoch:2,batch:200]: loss(train): 1.955, acc(train): 0.312, loss(val): 1.858, acc(val): 0.320
[epoch:2,batch:300]: loss(train): 1.820, acc(train): 0.336, loss(val): 1.697, acc(val): 0.365
Starting epoch 3
[epoch:3,batch:100]: loss(train): 1.733, acc(train): 0.422, loss(val): 1.649, acc(val): 0.382
[epoch:3,batch:200]: loss(train): 1.689, acc(train): 0.406, loss(val): 1.629, acc(val): 0.394
[epoch:3,batch:300]: loss(train): 1.635, acc(train): 0.398, loss(val): 1.575, acc(val): 0.414
Starting epoch 4
[epoch:4,batch:100]: loss(train): 1.567, acc(train): 0.445, loss(val): 1.473, acc(val): 0.449
[epoch:4,batch:200]: loss(train): 1.588, acc(train): 0.445, loss(val): 1.558, acc(val): 0.418
[epoch:4,batch:300]: loss(train): 1.517, acc(train): 0.406, loss(val): 1.481, acc(val): 0.444
Starting epoch 5
[epoch:5,batch:100]: loss(train): 1.467, acc(train): 0.523, loss(val): 1.386, acc(val): 0.485
[epoch:5,batch:200]: loss(train): 1.485, acc(train): 0.508, loss(val): 1.396, acc(val): 0.486
[epoch:5,batch:300]: loss(train): 1.483, acc(train): 0.477, loss(val): 1.438, acc(val): 0.467
Starting epoch 6
[epoch:6,batch:100]: loss(train): 1.396, acc(train): 0.562, loss(val): 1.321, acc(val): 0.509
[epoch:6,batch:200]: loss(train): 1.406, acc(train): 0.469, loss(val): 1.433, acc(val): 0.471
[epoch:6,batch:300]: loss(train): 1.379, acc(train): 0.453, loss(val): 1.378, acc(val): 0.493
Starting epoch 7
[epoch:7,batch:100]: loss(train): 1.319, acc(train): 0.516, loss(val): 1.261, acc(val): 0.536
[epoch:7,batch:200]: loss(train): 1.348, acc(train): 0.508, loss(val): 1.329, acc(val): 0.518
[epoch:7,batch:300]: loss(train): 1.332, acc(train): 0.492, loss(val): 1.246, acc(val): 0.547
Starting epoch 8
[epoch:8,batch:100]: loss(train): 1.257, acc(train): 0.500, loss(val): 1.217, acc(val): 0.558
[epoch:8,batch:200]: loss(train): 1.297, acc(train): 0.578, loss(val): 1.241, acc(val): 0.540
[epoch:8,batch:300]: loss(train): 1.274, acc(train): 0.641, loss(val): 1.176, acc(val): 0.569
Starting epoch 9
[epoch:9,batch:100]: loss(train): 1.181, acc(train): 0.578, loss(val): 1.134, acc(val): 0.587
[epoch:9,batch:200]: loss(train): 1.233, acc(train): 0.492, loss(val): 1.207, acc(val): 0.560
[epoch:9,batch:300]: loss(train): 1.212, acc(train): 0.562, loss(val): 1.178, acc(val): 0.575
Starting epoch 10
[epoch:10,batch:100]: loss(train): 1.132, acc(train): 0.492, loss(val): 1.115, acc(val): 0.596
[epoch:10,batch:200]: loss(train): 1.158, acc(train): 0.609, loss(val): 1.150, acc(val): 0.586
[epoch:10,batch:300]: loss(train): 1.151, acc(train): 0.578, loss(val): 1.143, acc(val): 0.589
Starting epoch 11
[epoch:11,batch:100]: loss(train): 1.045, acc(train): 0.609, loss(val): 1.052, acc(val): 0.624
[epoch:11,batch:200]: loss(train): 1.121, acc(train): 0.578, loss(val): 1.090, acc(val): 0.609
[epoch:11,batch:300]: loss(train): 1.104, acc(train): 0.523, loss(val): 1.048, acc(val): 0.624
Starting epoch 12
[epoch:12,batch:100]: loss(train): 1.002, acc(train): 0.633, loss(val): 0.995, acc(val): 0.643
[epoch:12,batch:200]: loss(train): 1.085, acc(train): 0.617, loss(val): 1.032, acc(val): 0.634
[epoch:12,batch:300]: loss(train): 1.064, acc(train): 0.656, loss(val): 1.001, acc(val): 0.639
Starting epoch 13
[epoch:13,batch:100]: loss(train): 0.962, acc(train): 0.562, loss(val): 0.940, acc(val): 0.672
[epoch:13,batch:200]: loss(train): 1.026, acc(train): 0.570, loss(val): 0.982, acc(val): 0.651
[epoch:13,batch:300]: loss(train): 1.014, acc(train): 0.648, loss(val): 0.976, acc(val): 0.652
Starting epoch 14
[epoch:14,batch:100]: loss(train): 0.919, acc(train): 0.594, loss(val): 0.907, acc(val): 0.679
[epoch:14,batch:200]: loss(train): 0.965, acc(train): 0.648, loss(val): 0.984, acc(val): 0.651
[epoch:14,batch:300]: loss(train): 0.962, acc(train): 0.727, loss(val): 1.030, acc(val): 0.645
Starting epoch 15
[epoch:15,batch:100]: loss(train): 0.870, acc(train): 0.719, loss(val): 0.884, acc(val): 0.689
[epoch:15,batch:200]: loss(train): 0.967, acc(train): 0.695, loss(val): 0.903, acc(val): 0.675
[epoch:15,batch:300]: loss(train): 0.919, acc(train): 0.633, loss(val): 0.915, acc(val): 0.677
Starting epoch 16
[epoch:16,batch:100]: loss(train): 0.850, acc(train): 0.656, loss(val): 0.871, acc(val): 0.692
[epoch:16,batch:200]: loss(train): 0.890, acc(train): 0.695, loss(val): 0.925, acc(val): 0.672
[epoch:16,batch:300]: loss(train): 0.895, acc(train): 0.617, loss(val): 0.893, acc(val): 0.685
Starting epoch 17
[epoch:17,batch:100]: loss(train): 0.810, acc(train): 0.703, loss(val): 0.856, acc(val): 0.697
[epoch:17,batch:200]: loss(train): 0.879, acc(train): 0.727, loss(val): 0.845, acc(val): 0.705
[epoch:17,batch:300]: loss(train): 0.869, acc(train): 0.672, loss(val): 0.847, acc(val): 0.700
Starting epoch 18
[epoch:18,batch:100]: loss(train): 0.790, acc(train): 0.656, loss(val): 0.795, acc(val): 0.724
[epoch:18,batch:200]: loss(train): 0.828, acc(train): 0.719, loss(val): 0.888, acc(val): 0.694
[epoch:18,batch:300]: loss(train): 0.823, acc(train): 0.711, loss(val): 0.867, acc(val): 0.691
Starting epoch 19
[epoch:19,batch:100]: loss(train): 0.757, acc(train): 0.781, loss(val): 0.781, acc(val): 0.728
[epoch:19,batch:200]: loss(train): 0.811, acc(train): 0.719, loss(val): 0.830, acc(val): 0.706
[epoch:19,batch:300]: loss(train): 0.835, acc(train): 0.648, loss(val): 0.893, acc(val): 0.702
Starting epoch 20
[epoch:20,batch:100]: loss(train): 0.753, acc(train): 0.750, loss(val): 0.774, acc(val): 0.725
[epoch:20,batch:200]: loss(train): 0.791, acc(train): 0.672, loss(val): 0.796, acc(val): 0.721
[epoch:20,batch:300]: loss(train): 0.788, acc(train): 0.727, loss(val): 0.781, acc(val): 0.730
Starting epoch 21
[epoch:21,batch:100]: loss(train): 0.718, acc(train): 0.703, loss(val): 0.740, acc(val): 0.739
[epoch:21,batch:200]: loss(train): 0.791, acc(train): 0.711, loss(val): 0.786, acc(val): 0.720
[epoch:21,batch:300]: loss(train): 0.736, acc(train): 0.719, loss(val): 0.826, acc(val): 0.711
Starting epoch 22
[epoch:22,batch:100]: loss(train): 0.692, acc(train): 0.727, loss(val): 0.727, acc(val): 0.741
[epoch:22,batch:200]: loss(train): 0.765, acc(train): 0.656, loss(val): 0.901, acc(val): 0.687
[epoch:22,batch:300]: loss(train): 0.754, acc(train): 0.766, loss(val): 0.760, acc(val): 0.731
Starting epoch 23
[epoch:23,batch:100]: loss(train): 0.670, acc(train): 0.742, loss(val): 0.705, acc(val): 0.755
[epoch:23,batch:200]: loss(train): 0.705, acc(train): 0.805, loss(val): 0.793, acc(val): 0.722
[epoch:23,batch:300]: loss(train): 0.730, acc(train): 0.680, loss(val): 0.757, acc(val): 0.736
Starting epoch 24
[epoch:24,batch:100]: loss(train): 0.647, acc(train): 0.812, loss(val): 0.685, acc(val): 0.764
[epoch:24,batch:200]: loss(train): 0.722, acc(train): 0.820, loss(val): 0.725, acc(val): 0.751
[epoch:24,batch:300]: loss(train): 0.691, acc(train): 0.695, loss(val): 0.809, acc(val): 0.724
Starting epoch 25
[epoch:25,batch:100]: loss(train): 0.628, acc(train): 0.797, loss(val): 0.676, acc(val): 0.768
[epoch:25,batch:200]: loss(train): 0.674, acc(train): 0.766, loss(val): 0.771, acc(val): 0.738
[epoch:25,batch:300]: loss(train): 0.690, acc(train): 0.711, loss(val): 0.729, acc(val): 0.746
Starting epoch 26
[epoch:26,batch:100]: loss(train): 0.626, acc(train): 0.688, loss(val): 0.661, acc(val): 0.774
[epoch:26,batch:200]: loss(train): 0.678, acc(train): 0.781, loss(val): 0.771, acc(val): 0.736
[epoch:26,batch:300]: loss(train): 0.685, acc(train): 0.719, loss(val): 0.707, acc(val): 0.752
Starting epoch 27
[epoch:27,batch:100]: loss(train): 0.606, acc(train): 0.766, loss(val): 0.651, acc(val): 0.769
[epoch:27,batch:200]: loss(train): 0.659, acc(train): 0.758, loss(val): 0.770, acc(val): 0.739
[epoch:27,batch:300]: loss(train): 0.678, acc(train): 0.711, loss(val): 0.740, acc(val): 0.745
Starting epoch 28
[epoch:28,batch:100]: loss(train): 0.586, acc(train): 0.805, loss(val): 0.638, acc(val): 0.780
[epoch:28,batch:200]: loss(train): 0.655, acc(train): 0.766, loss(val): 0.734, acc(val): 0.746
[epoch:28,batch:300]: loss(train): 0.649, acc(train): 0.867, loss(val): 0.664, acc(val): 0.764
Starting epoch 29
[epoch:29,batch:100]: loss(train): 0.577, acc(train): 0.812, loss(val): 0.628, acc(val): 0.781
[epoch:29,batch:200]: loss(train): 0.645, acc(train): 0.750, loss(val): 0.691, acc(val): 0.763
[epoch:29,batch:300]: loss(train): 0.627, acc(train): 0.703, loss(val): 0.683, acc(val): 0.765
Starting epoch 30
[epoch:30,batch:100]: loss(train): 0.538, acc(train): 0.836, loss(val): 0.620, acc(val): 0.784
[epoch:30,batch:200]: loss(train): 0.622, acc(train): 0.773, loss(val): 0.648, acc(val): 0.771
[epoch:30,batch:300]: loss(train): 0.612, acc(train): 0.797, loss(val): 0.681, acc(val): 0.768
Starting epoch 31
[epoch:31,batch:100]: loss(train): 0.539, acc(train): 0.820, loss(val): 0.631, acc(val): 0.781
[epoch:31,batch:200]: loss(train): 0.625, acc(train): 0.773, loss(val): 0.696, acc(val): 0.762
[epoch:31,batch:300]: loss(train): 0.611, acc(train): 0.812, loss(val): 0.677, acc(val): 0.767
Starting epoch 32
[epoch:32,batch:100]: loss(train): 0.523, acc(train): 0.812, loss(val): 0.597, acc(val): 0.794
[epoch:32,batch:200]: loss(train): 0.582, acc(train): 0.797, loss(val): 0.663, acc(val): 0.775
[epoch:32,batch:300]: loss(train): 0.584, acc(train): 0.781, loss(val): 0.685, acc(val): 0.762
Starting epoch 33
[epoch:33,batch:100]: loss(train): 0.513, acc(train): 0.789, loss(val): 0.594, acc(val): 0.794
[epoch:33,batch:200]: loss(train): 0.572, acc(train): 0.742, loss(val): 0.674, acc(val): 0.772
[epoch:33,batch:300]: loss(train): 0.593, acc(train): 0.789, loss(val): 0.659, acc(val): 0.780
Starting epoch 34
[epoch:34,batch:100]: loss(train): 0.507, acc(train): 0.828, loss(val): 0.588, acc(val): 0.799
[epoch:34,batch:200]: loss(train): 0.565, acc(train): 0.773, loss(val): 0.644, acc(val): 0.778
[epoch:34,batch:300]: loss(train): 0.565, acc(train): 0.773, loss(val): 0.672, acc(val): 0.772
Starting epoch 35
[epoch:35,batch:100]: loss(train): 0.513, acc(train): 0.836, loss(val): 0.590, acc(val): 0.800
[epoch:35,batch:200]: loss(train): 0.554, acc(train): 0.781, loss(val): 0.674, acc(val): 0.772
[epoch:35,batch:300]: loss(train): 0.564, acc(train): 0.797, loss(val): 0.633, acc(val): 0.787
Starting epoch 36
[epoch:36,batch:100]: loss(train): 0.497, acc(train): 0.828, loss(val): 0.558, acc(val): 0.806
[epoch:36,batch:200]: loss(train): 0.541, acc(train): 0.859, loss(val): 0.652, acc(val): 0.774
[epoch:36,batch:300]: loss(train): 0.537, acc(train): 0.828, loss(val): 0.646, acc(val): 0.783
Starting epoch 37
[epoch:37,batch:100]: loss(train): 0.479, acc(train): 0.859, loss(val): 0.571, acc(val): 0.804
[epoch:37,batch:200]: loss(train): 0.517, acc(train): 0.750, loss(val): 0.661, acc(val): 0.775
[epoch:37,batch:300]: loss(train): 0.564, acc(train): 0.812, loss(val): 0.623, acc(val): 0.786
Starting epoch 38
[epoch:38,batch:100]: loss(train): 0.472, acc(train): 0.820, loss(val): 0.551, acc(val): 0.809
[epoch:38,batch:200]: loss(train): 0.508, acc(train): 0.812, loss(val): 0.596, acc(val): 0.799
[epoch:38,batch:300]: loss(train): 0.543, acc(train): 0.797, loss(val): 0.614, acc(val): 0.788
Starting epoch 39
[epoch:39,batch:100]: loss(train): 0.460, acc(train): 0.812, loss(val): 0.537, acc(val): 0.819
[epoch:39,batch:200]: loss(train): 0.505, acc(train): 0.812, loss(val): 0.581, acc(val): 0.802
[epoch:39,batch:300]: loss(train): 0.519, acc(train): 0.836, loss(val): 0.639, acc(val): 0.789
Starting epoch 40
[epoch:40,batch:100]: loss(train): 0.449, acc(train): 0.867, loss(val): 0.537, acc(val): 0.814
[epoch:40,batch:200]: loss(train): 0.496, acc(train): 0.820, loss(val): 0.556, acc(val): 0.811
[epoch:40,batch:300]: loss(train): 0.497, acc(train): 0.852, loss(val): 0.676, acc(val): 0.774
Starting epoch 41
[epoch:41,batch:100]: loss(train): 0.443, acc(train): 0.906, loss(val): 0.531, acc(val): 0.819
[epoch:41,batch:200]: loss(train): 0.476, acc(train): 0.828, loss(val): 0.601, acc(val): 0.799
[epoch:41,batch:300]: loss(train): 0.501, acc(train): 0.844, loss(val): 0.555, acc(val): 0.811
Starting epoch 42
[epoch:42,batch:100]: loss(train): 0.436, acc(train): 0.828, loss(val): 0.530, acc(val): 0.818
[epoch:42,batch:200]: loss(train): 0.482, acc(train): 0.852, loss(val): 0.584, acc(val): 0.801
[epoch:42,batch:300]: loss(train): 0.492, acc(train): 0.797, loss(val): 0.603, acc(val): 0.796
Starting epoch 43
[epoch:43,batch:100]: loss(train): 0.421, acc(train): 0.883, loss(val): 0.526, acc(val): 0.820
[epoch:43,batch:200]: loss(train): 0.446, acc(train): 0.742, loss(val): 0.575, acc(val): 0.802
[epoch:43,batch:300]: loss(train): 0.487, acc(train): 0.805, loss(val): 0.576, acc(val): 0.806
Starting epoch 44
[epoch:44,batch:100]: loss(train): 0.410, acc(train): 0.820, loss(val): 0.509, acc(val): 0.826
[epoch:44,batch:200]: loss(train): 0.476, acc(train): 0.852, loss(val): 0.588, acc(val): 0.801
[epoch:44,batch:300]: loss(train): 0.461, acc(train): 0.859, loss(val): 0.539, acc(val): 0.814
Starting epoch 45
[epoch:45,batch:100]: loss(train): 0.398, acc(train): 0.891, loss(val): 0.513, acc(val): 0.825
[epoch:45,batch:200]: loss(train): 0.457, acc(train): 0.852, loss(val): 0.609, acc(val): 0.795
[epoch:45,batch:300]: loss(train): 0.452, acc(train): 0.875, loss(val): 0.542, acc(val): 0.817
Starting epoch 46
[epoch:46,batch:100]: loss(train): 0.394, acc(train): 0.844, loss(val): 0.505, acc(val): 0.828
[epoch:46,batch:200]: loss(train): 0.465, acc(train): 0.828, loss(val): 0.571, acc(val): 0.810
[epoch:46,batch:300]: loss(train): 0.446, acc(train): 0.859, loss(val): 0.547, acc(val): 0.812
Starting epoch 47
[epoch:47,batch:100]: loss(train): 0.386, acc(train): 0.867, loss(val): 0.502, acc(val): 0.830
[epoch:47,batch:200]: loss(train): 0.424, acc(train): 0.844, loss(val): 0.538, acc(val): 0.820
[epoch:47,batch:300]: loss(train): 0.459, acc(train): 0.781, loss(val): 0.542, acc(val): 0.820
Starting epoch 48
[epoch:48,batch:100]: loss(train): 0.392, acc(train): 0.883, loss(val): 0.507, acc(val): 0.828
[epoch:48,batch:200]: loss(train): 0.421, acc(train): 0.883, loss(val): 0.561, acc(val): 0.815
[epoch:48,batch:300]: loss(train): 0.446, acc(train): 0.789, loss(val): 0.539, acc(val): 0.818
Starting epoch 49
[epoch:49,batch:100]: loss(train): 0.375, acc(train): 0.844, loss(val): 0.495, acc(val): 0.831
[epoch:49,batch:200]: loss(train): 0.437, acc(train): 0.883, loss(val): 0.534, acc(val): 0.823
[epoch:49,batch:300]: loss(train): 0.438, acc(train): 0.891, loss(val): 0.514, acc(val): 0.827
Starting epoch 50
[epoch:50,batch:100]: loss(train): 0.356, acc(train): 0.875, loss(val): 0.499, acc(val): 0.834
[epoch:50,batch:200]: loss(train): 0.409, acc(train): 0.828, loss(val): 0.526, acc(val): 0.827
[epoch:50,batch:300]: loss(train): 0.415, acc(train): 0.867, loss(val): 0.534, acc(val): 0.820
Starting epoch 51
[epoch:51,batch:100]: loss(train): 0.352, acc(train): 0.875, loss(val): 0.497, acc(val): 0.833
[epoch:51,batch:200]: loss(train): 0.424, acc(train): 0.914, loss(val): 0.511, acc(val): 0.831
[epoch:51,batch:300]: loss(train): 0.406, acc(train): 0.859, loss(val): 0.530, acc(val): 0.826
Starting epoch 52
[epoch:52,batch:100]: loss(train): 0.334, acc(train): 0.875, loss(val): 0.470, acc(val): 0.843
[epoch:52,batch:200]: loss(train): 0.401, acc(train): 0.789, loss(val): 0.539, acc(val): 0.823
[epoch:52,batch:300]: loss(train): 0.417, acc(train): 0.875, loss(val): 0.533, acc(val): 0.825
Starting epoch 53
[epoch:53,batch:100]: loss(train): 0.342, acc(train): 0.906, loss(val): 0.482, acc(val): 0.840
[epoch:53,batch:200]: loss(train): 0.392, acc(train): 0.844, loss(val): 0.532, acc(val): 0.819
[epoch:53,batch:300]: loss(train): 0.406, acc(train): 0.820, loss(val): 0.503, acc(val): 0.829
Starting epoch 54
[epoch:54,batch:100]: loss(train): 0.325, acc(train): 0.844, loss(val): 0.468, acc(val): 0.844
[epoch:54,batch:200]: loss(train): 0.378, acc(train): 0.891, loss(val): 0.509, acc(val): 0.832
[epoch:54,batch:300]: loss(train): 0.398, acc(train): 0.836, loss(val): 0.567, acc(val): 0.818
Starting epoch 55
[epoch:55,batch:100]: loss(train): 0.333, acc(train): 0.875, loss(val): 0.476, acc(val): 0.841
[epoch:55,batch:200]: loss(train): 0.389, acc(train): 0.805, loss(val): 0.585, acc(val): 0.808
[epoch:55,batch:300]: loss(train): 0.397, acc(train): 0.898, loss(val): 0.525, acc(val): 0.826
Starting epoch 56
[epoch:56,batch:100]: loss(train): 0.330, acc(train): 0.898, loss(val): 0.477, acc(val): 0.840
[epoch:56,batch:200]: loss(train): 0.371, acc(train): 0.844, loss(val): 0.506, acc(val): 0.830
[epoch:56,batch:300]: loss(train): 0.375, acc(train): 0.859, loss(val): 0.530, acc(val): 0.823
Starting epoch 57
[epoch:57,batch:100]: loss(train): 0.324, acc(train): 0.906, loss(val): 0.459, acc(val): 0.845
[epoch:57,batch:200]: loss(train): 0.374, acc(train): 0.844, loss(val): 0.575, acc(val): 0.818
[epoch:57,batch:300]: loss(train): 0.383, acc(train): 0.859, loss(val): 0.496, acc(val): 0.832
Starting epoch 58
[epoch:58,batch:100]: loss(train): 0.315, acc(train): 0.883, loss(val): 0.464, acc(val): 0.843
[epoch:58,batch:200]: loss(train): 0.358, acc(train): 0.883, loss(val): 0.560, acc(val): 0.824
[epoch:58,batch:300]: loss(train): 0.369, acc(train): 0.898, loss(val): 0.534, acc(val): 0.828
Starting epoch 59
