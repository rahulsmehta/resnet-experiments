Lottery Ticket Experiment for <function ResNet18 at 0x7f7f57976598> with pruning rate 10.737418240000006 on device cuda:2
Training base network...
Starting epoch 1
[epoch:1,batch:100]: loss(train): 0.950, acc(train): 0.734, loss(val): 1.803, acc(val): 0.469
Starting epoch 2
[epoch:2,batch:100]: loss(train): nan, acc(train): 0.242, loss(val): nan, acc(val): 0.200
Starting epoch 3
[epoch:3,batch:100]: loss(train): nan, acc(train): 0.195, loss(val): nan, acc(val): 0.200
Starting epoch 4
[epoch:4,batch:100]: loss(train): nan, acc(train): 0.188, loss(val): nan, acc(val): 0.200
Starting epoch 5
[epoch:5,batch:100]: loss(train): nan, acc(train): 0.219, loss(val): nan, acc(val): 0.200
Starting epoch 6
[epoch:6,batch:100]: loss(train): nan, acc(train): 0.180, loss(val): nan, acc(val): 0.200
Starting epoch 7
[epoch:7,batch:100]: loss(train): nan, acc(train): 0.164, loss(val): nan, acc(val): 0.200
Starting epoch 8
[epoch:8,batch:100]: loss(train): nan, acc(train): 0.297, loss(val): nan, acc(val): 0.200
Starting epoch 9
[epoch:9,batch:100]: loss(train): nan, acc(train): 0.227, loss(val): nan, acc(val): 0.200
Starting epoch 10
[epoch:10,batch:100]: loss(train): nan, acc(train): 0.211, loss(val): nan, acc(val): 0.200
Starting epoch 11
[epoch:11,batch:100]: loss(train): nan, acc(train): 0.203, loss(val): nan, acc(val): 0.200
Starting epoch 12
[epoch:12,batch:100]: loss(train): nan, acc(train): 0.172, loss(val): nan, acc(val): 0.200
Starting epoch 13
[epoch:13,batch:100]: loss(train): nan, acc(train): 0.227, loss(val): nan, acc(val): 0.200
Starting epoch 14
[epoch:14,batch:100]: loss(train): nan, acc(train): 0.219, loss(val): nan, acc(val): 0.200
Starting epoch 15
[epoch:15,batch:100]: loss(train): nan, acc(train): 0.156, loss(val): nan, acc(val): 0.200
Starting epoch 16
[epoch:16,batch:100]: loss(train): nan, acc(train): 0.203, loss(val): nan, acc(val): 0.200
Starting epoch 17
[epoch:17,batch:100]: loss(train): nan, acc(train): 0.180, loss(val): nan, acc(val): 0.200
Starting epoch 18
[epoch:18,batch:100]: loss(train): nan, acc(train): 0.188, loss(val): nan, acc(val): 0.200
Starting epoch 19
[epoch:19,batch:100]: loss(train): nan, acc(train): 0.188, loss(val): nan, acc(val): 0.200
Starting epoch 20
[epoch:20,batch:100]: loss(train): nan, acc(train): 0.219, loss(val): nan, acc(val): 0.200
Starting epoch 21
[epoch:21,batch:100]: loss(train): nan, acc(train): 0.156, loss(val): nan, acc(val): 0.200
Starting epoch 22
[epoch:22,batch:100]: loss(train): nan, acc(train): 0.180, loss(val): nan, acc(val): 0.200
Starting epoch 23
[epoch:23,batch:100]: loss(train): nan, acc(train): 0.227, loss(val): nan, acc(val): 0.200
Starting epoch 24
[epoch:24,batch:100]: loss(train): nan, acc(train): 0.172, loss(val): nan, acc(val): 0.200
Starting epoch 25
[epoch:25,batch:100]: loss(train): nan, acc(train): 0.211, loss(val): nan, acc(val): 0.200
Starting epoch 26
[epoch:26,batch:100]: loss(train): nan, acc(train): 0.133, loss(val): nan, acc(val): 0.200
Starting epoch 27
[epoch:27,batch:100]: loss(train): nan, acc(train): 0.195, loss(val): nan, acc(val): 0.200
Starting epoch 28
[epoch:28,batch:100]: loss(train): nan, acc(train): 0.180, loss(val): nan, acc(val): 0.200
Starting epoch 29
[epoch:29,batch:100]: loss(train): nan, acc(train): 0.180, loss(val): nan, acc(val): 0.200
Starting epoch 30
[epoch:30,batch:100]: loss(train): nan, acc(train): 0.227, loss(val): nan, acc(val): 0.200
Starting epoch 31
[epoch:31,batch:100]: loss(train): nan, acc(train): 0.195, loss(val): nan, acc(val): 0.200
Starting epoch 32
[epoch:32,batch:100]: loss(train): nan, acc(train): 0.211, loss(val): nan, acc(val): 0.200
Starting epoch 33
[epoch:33,batch:100]: loss(train): nan, acc(train): 0.242, loss(val): nan, acc(val): 0.200
Starting epoch 34
[epoch:34,batch:100]: loss(train): nan, acc(train): 0.188, loss(val): nan, acc(val): 0.200
Starting epoch 35
[epoch:35,batch:100]: loss(train): nan, acc(train): 0.195, loss(val): nan, acc(val): 0.200
Starting epoch 36
[epoch:36,batch:100]: loss(train): nan, acc(train): 0.133, loss(val): nan, acc(val): 0.200
Starting epoch 37
[epoch:37,batch:100]: loss(train): nan, acc(train): 0.219, loss(val): nan, acc(val): 0.200
Starting epoch 38
[epoch:38,batch:100]: loss(train): nan, acc(train): 0.180, loss(val): nan, acc(val): 0.200
Starting epoch 39
[epoch:39,batch:100]: loss(train): nan, acc(train): 0.219, loss(val): nan, acc(val): 0.200
Starting epoch 40
[epoch:40,batch:100]: loss(train): nan, acc(train): 0.227, loss(val): nan, acc(val): 0.200
Starting epoch 41
[epoch:41,batch:100]: loss(train): nan, acc(train): 0.172, loss(val): nan, acc(val): 0.200
Starting epoch 42
[epoch:42,batch:100]: loss(train): nan, acc(train): 0.289, loss(val): nan, acc(val): 0.200
Starting epoch 43
[epoch:43,batch:100]: loss(train): nan, acc(train): 0.305, loss(val): nan, acc(val): 0.200
Starting epoch 44
[epoch:44,batch:100]: loss(train): nan, acc(train): 0.211, loss(val): nan, acc(val): 0.200
Starting epoch 45
[epoch:45,batch:100]: loss(train): nan, acc(train): 0.203, loss(val): nan, acc(val): 0.200
Starting epoch 46
[epoch:46,batch:100]: loss(train): nan, acc(train): 0.219, loss(val): nan, acc(val): 0.200
Starting epoch 47
[epoch:47,batch:100]: loss(train): nan, acc(train): 0.148, loss(val): nan, acc(val): 0.200
Starting epoch 48
[epoch:48,batch:100]: loss(train): nan, acc(train): 0.203, loss(val): nan, acc(val): 0.200
Starting epoch 49
[epoch:49,batch:100]: loss(train): nan, acc(train): 0.227, loss(val): nan, acc(val): 0.200
Starting epoch 50
[epoch:50,batch:100]: loss(train): nan, acc(train): 0.219, loss(val): nan, acc(val): 0.200
Starting epoch 51
[epoch:51,batch:100]: loss(train): nan, acc(train): 0.297, loss(val): nan, acc(val): 0.200
Starting epoch 52
[epoch:52,batch:100]: loss(train): nan, acc(train): 0.195, loss(val): nan, acc(val): 0.200
Starting epoch 53
[epoch:53,batch:100]: loss(train): nan, acc(train): 0.219, loss(val): nan, acc(val): 0.200
Starting epoch 54
[epoch:54,batch:100]: loss(train): nan, acc(train): 0.141, loss(val): nan, acc(val): 0.200
Starting epoch 55
[epoch:55,batch:100]: loss(train): nan, acc(train): 0.211, loss(val): nan, acc(val): 0.200
Starting epoch 56
[epoch:56,batch:100]: loss(train): nan, acc(train): 0.180, loss(val): nan, acc(val): 0.200
Starting epoch 57
[epoch:57,batch:100]: loss(train): nan, acc(train): 0.211, loss(val): nan, acc(val): 0.200
Starting epoch 58
[epoch:58,batch:100]: loss(train): nan, acc(train): 0.250, loss(val): nan, acc(val): 0.200
Starting epoch 59
[epoch:59,batch:100]: loss(train): nan, acc(train): 0.156, loss(val): nan, acc(val): 0.200
Starting epoch 60
[epoch:60,batch:100]: loss(train): nan, acc(train): 0.203, loss(val): nan, acc(val): 0.200
Starting epoch 61
[epoch:61,batch:100]: loss(train): nan, acc(train): 0.203, loss(val): nan, acc(val): 0.200
Starting epoch 62
[epoch:62,batch:100]: loss(train): nan, acc(train): 0.242, loss(val): nan, acc(val): 0.200
Starting epoch 63
[epoch:63,batch:100]: loss(train): nan, acc(train): 0.195, loss(val): nan, acc(val): 0.200
Starting epoch 64
[epoch:64,batch:100]: loss(train): nan, acc(train): 0.250, loss(val): nan, acc(val): 0.200
Starting epoch 65
[epoch:65,batch:100]: loss(train): nan, acc(train): 0.227, loss(val): nan, acc(val): 0.200
Starting epoch 66
[epoch:66,batch:100]: loss(train): nan, acc(train): 0.156, loss(val): nan, acc(val): 0.200
Starting epoch 67
[epoch:67,batch:100]: loss(train): nan, acc(train): 0.234, loss(val): nan, acc(val): 0.200
Starting epoch 68
[epoch:68,batch:100]: loss(train): nan, acc(train): 0.227, loss(val): nan, acc(val): 0.200
Starting epoch 69
[epoch:69,batch:100]: loss(train): nan, acc(train): 0.203, loss(val): nan, acc(val): 0.200
Starting epoch 70
[epoch:70,batch:100]: loss(train): nan, acc(train): 0.211, loss(val): nan, acc(val): 0.200
Starting epoch 71
[epoch:71,batch:100]: loss(train): nan, acc(train): 0.188, loss(val): nan, acc(val): 0.200
Starting epoch 72
[epoch:72,batch:100]: loss(train): nan, acc(train): 0.172, loss(val): nan, acc(val): 0.200
Starting epoch 73
[epoch:73,batch:100]: loss(train): nan, acc(train): 0.180, loss(val): nan, acc(val): 0.200
Starting epoch 74
[epoch:74,batch:100]: loss(train): nan, acc(train): 0.195, loss(val): nan, acc(val): 0.200
Starting epoch 75
[epoch:75,batch:100]: loss(train): nan, acc(train): 0.227, loss(val): nan, acc(val): 0.200
Starting epoch 76
[epoch:76,batch:100]: loss(train): nan, acc(train): 0.219, loss(val): nan, acc(val): 0.200
Starting epoch 77
[epoch:77,batch:100]: loss(train): nan, acc(train): 0.211, loss(val): nan, acc(val): 0.200
Starting epoch 78
[epoch:78,batch:100]: loss(train): nan, acc(train): 0.195, loss(val): nan, acc(val): 0.200
Starting epoch 79
[epoch:79,batch:100]: loss(train): nan, acc(train): 0.211, loss(val): nan, acc(val): 0.200
Starting epoch 80
[epoch:80,batch:100]: loss(train): nan, acc(train): 0.180, loss(val): nan, acc(val): 0.200
Starting epoch 81
[epoch:81,batch:100]: loss(train): nan, acc(train): 0.180, loss(val): nan, acc(val): 0.200
Starting epoch 82
[epoch:82,batch:100]: loss(train): nan, acc(train): 0.172, loss(val): nan, acc(val): 0.200
Starting epoch 83
[epoch:83,batch:100]: loss(train): nan, acc(train): 0.219, loss(val): nan, acc(val): 0.200
Starting epoch 84
[epoch:84,batch:100]: loss(train): nan, acc(train): 0.211, loss(val): nan, acc(val): 0.200
Starting epoch 85
[epoch:85,batch:100]: loss(train): nan, acc(train): 0.211, loss(val): nan, acc(val): 0.200
Starting epoch 86
[epoch:86,batch:100]: loss(train): nan, acc(train): 0.195, loss(val): nan, acc(val): 0.200
Starting epoch 87
[epoch:87,batch:100]: loss(train): nan, acc(train): 0.227, loss(val): nan, acc(val): 0.200
Starting epoch 88
[epoch:88,batch:100]: loss(train): nan, acc(train): 0.180, loss(val): nan, acc(val): 0.200
Starting epoch 89
[epoch:89,batch:100]: loss(train): nan, acc(train): 0.156, loss(val): nan, acc(val): 0.200
Starting epoch 90
[epoch:90,batch:100]: loss(train): nan, acc(train): 0.180, loss(val): nan, acc(val): 0.200
Starting epoch 91
